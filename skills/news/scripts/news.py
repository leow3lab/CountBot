#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""ä¸­æ–‡æ–°é—»èšåˆå·¥å…· - åŸºäº RSS æºå’Œç½‘é¡µæŠ“å–ï¼Œçº¯æ ‡å‡†åº“å®ç°"""
import argparse
import sys
import json
import os
import re
import io
import html
import time
import urllib.request
import urllib.error
from xml.etree import ElementTree as ET
from datetime import datetime

if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

UA = "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_7) AppleWebKit/537.36 Chrome/131.0 Safari/537.36"
TIMEOUT = 8

# â”€â”€ æ–°é—»æºå®šä¹‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SOURCES = {
    "hot": [
        {"name": "äººæ°‘ç½‘",   "type": "rss", "url": "http://www.people.com.cn/rss/politics.xml"},
        {"name": "æ–°åç½‘",   "type": "web", "url": "http://www.news.cn/politics/"},
        {"name": "æ¾æ¹ƒæ–°é—»", "type": "rss", "url": "https://www.thepaper.cn/rss_newslist_all.xml"},
    ],
    "politics": [
        {"name": "äººæ°‘ç½‘æ—¶æ”¿", "type": "rss", "url": "http://www.people.com.cn/rss/politics.xml"},
        {"name": "æ–°åç½‘æ”¿åŠ¡", "type": "web", "url": "http://www.news.cn/politics/"},
    ],
    "finance": [
        {"name": "æ–°æµªè´¢ç»",  "type": "rss", "url": "https://feed.mix.sina.com.cn/api/roll/get?pageid=153&lid=2516&k=&num=20&page=1&r=0.1&callback="},
        {"name": "ä¸œæ–¹è´¢å¯Œ",  "type": "web", "url": "https://www.eastmoney.com/"},
    ],
    "tech": [
        {"name": "36æ°ª",     "type": "rss", "url": "https://36kr.com/feed"},
        {"name": "ITä¹‹å®¶",   "type": "rss", "url": "https://www.ithome.com/rss/"},
    ],
    "society": [
        {"name": "ä¸­å›½æ–°é—»ç½‘", "type": "rss", "url": "https://www.chinanews.com/rss/scroll-news.xml"},
        {"name": "æ¾æ¹ƒæ–°é—»",  "type": "rss", "url": "https://www.thepaper.cn/rss_newslist_all.xml"},
    ],
    "world": [
        {"name": "ç¯çƒç½‘",   "type": "web", "url": "https://world.huanqiu.com/"},
        {"name": "å‚è€ƒæ¶ˆæ¯",  "type": "web", "url": "http://www.cankaoxiaoxi.com/"},
    ],
    "sports": [
        {"name": "æ–°æµªä½“è‚²", "type": "web", "url": "https://sports.sina.com.cn/"},
        {"name": "è™æ‰‘",     "type": "web", "url": "https://www.hupu.com/"},
    ],
    "entertainment": [
        {"name": "æ–°æµªå¨±ä¹", "type": "web", "url": "https://ent.sina.com.cn/"},
    ],
    "ai": [
        {"name": "MIT Tech Review",  "type": "rss", "url": "https://www.technologyreview.com/feed/"},
        {"name": "OpenAI Blog",      "type": "rss", "url": "https://openai.com/blog/rss.xml"},
        {"name": "Google AI Blog",   "type": "rss", "url": "https://blog.google/technology/ai/rss/"},
        {"name": "DeepMind Blog",    "type": "rss", "url": "https://deepmind.google/blog/rss.xml"},
        {"name": "Latent Space",     "type": "rss", "url": "https://www.latent.space/feed"},
        {"name": "Interconnects",    "type": "rss", "url": "https://www.interconnects.ai/feed"},
        {"name": "One Useful Thing", "type": "rss", "url": "https://www.oneusefulthing.org/feed"},
        {"name": "KDnuggets",       "type": "rss", "url": "https://www.kdnuggets.com/feed"},
    ],
    "ai-community": [
        {"name": "AI News Daily",    "type": "rss", "url": "https://buttondown.com/ainews/rss"},
        {"name": "Sebastian Raschka","type": "rss", "url": "https://magazine.sebastianraschka.com/feed"},
        {"name": "Hacker News",      "type": "rss", "url": "https://hnrss.org/frontpage"},
        {"name": "Product Hunt",     "type": "rss", "url": "https://www.producthunt.com/feed"},
    ],
}

CAT_NAMES = {
    "hot": "çƒ­ç‚¹è¦é—»", "politics": "æ—¶æ”¿", "finance": "è´¢ç»",
    "tech": "ç§‘æŠ€", "society": "ç¤¾ä¼š", "world": "å›½é™…",
    "sports": "ä½“è‚²", "entertainment": "å¨±ä¹",
    "ai": "AI æŠ€æœ¯ä¸èµ„è®¯", "ai-community": "AI ç¤¾åŒºä¸èšåˆ",
}


# â”€â”€ ç½‘ç»œè¯·æ±‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch(url, timeout=TIMEOUT):
    """è·å– URL å†…å®¹ï¼Œè¿”å›æ–‡æœ¬"""
    req = urllib.request.Request(url, headers={"User-Agent": UA})
    try:
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            data = resp.read()
            # å°è¯•å¤šç§ç¼–ç 
            for enc in ('utf-8', 'gbk', 'gb2312', 'latin-1'):
                try:
                    return data.decode(enc)
                except (UnicodeDecodeError, LookupError):
                    continue
            return data.decode('utf-8', errors='replace')
    except Exception as e:
        print(f"  âš ï¸  è·å– {url} å¤±è´¥: {e}", file=sys.stderr)
        return None


# â”€â”€ RSS è§£æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_rss(xml_text, source_name):
    """è§£æ RSS/Atom XMLï¼Œè¿”å›æ–°é—»åˆ—è¡¨"""
    items = []
    try:
        root = ET.fromstring(xml_text)
    except ET.ParseError:
        return items

    # RSS 2.0
    for item in root.iter('item'):
        title = _text(item, 'title')
        link = _text(item, 'link')
        pub = _text(item, 'pubDate')
        desc = _text(item, 'description')
        # ä¼˜å…ˆå– content:encodedï¼ˆå®Œæ•´æ­£æ–‡ï¼‰ï¼Œå…¶æ¬¡ description
        content = None
        for tag in ('{http://purl.org/rss/1.0/modules/content/}encoded', 'content:encoded'):
            content = _text(item, tag)
            if content:
                break
        body = _clean(content) if content else (_clean(desc) if desc else "")
        if title and link:
            items.append({
                "title": _clean(title),
                "link": link.strip(),
                "source": source_name,
                "time": _parse_time(pub),
                "summary": body,
            })

    # Atom
    if not items:
        ns = {'atom': 'http://www.w3.org/2005/Atom'}
        for entry in root.iter('{http://www.w3.org/2005/Atom}entry'):
            title = _text(entry, '{http://www.w3.org/2005/Atom}title')
            link_el = entry.find('{http://www.w3.org/2005/Atom}link')
            link = link_el.get('href', '') if link_el is not None else ''
            pub = _text(entry, '{http://www.w3.org/2005/Atom}published') or _text(entry, '{http://www.w3.org/2005/Atom}updated')
            content = _text(entry, '{http://www.w3.org/2005/Atom}content') or _text(entry, '{http://www.w3.org/2005/Atom}summary') or ""
            if title and link:
                items.append({
                    "title": _clean(title),
                    "link": link.strip(),
                    "source": source_name,
                    "time": _parse_time(pub),
                    "summary": _clean(content),
                })
    return items


def _text(el, tag):
    """å®‰å…¨è·å–å­å…ƒç´ æ–‡æœ¬"""
    child = el.find(tag)
    return child.text if child is not None and child.text else None


def _clean(text):
    """æ¸…ç† HTML æ ‡ç­¾å’Œå¤šä½™ç©ºç™½"""
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', '', text)
    text = html.unescape(text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def _parse_time(time_str):
    """å°è¯•è§£ææ—¶é—´å­—ç¬¦ä¸²"""
    if not time_str:
        return ""
    # å¸¸è§ RSS æ—¶é—´æ ¼å¼
    for fmt in ('%a, %d %b %Y %H:%M:%S %z', '%a, %d %b %Y %H:%M:%S GMT',
                '%Y-%m-%dT%H:%M:%S%z', '%Y-%m-%dT%H:%M:%SZ',
                '%Y-%m-%d %H:%M:%S', '%Y-%m-%d'):
        try:
            dt = datetime.strptime(time_str.strip(), fmt)
            return dt.strftime('%m-%d %H:%M')
        except ValueError:
            continue
    return time_str[:16]


# â”€â”€ ç½‘é¡µæ ‡é¢˜æå– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_web_titles(html_text, source_name, base_url):
    """ä»ç½‘é¡µ HTML ä¸­æå–æ–°é—»æ ‡é¢˜å’Œé“¾æ¥"""
    items = []
    # åŒ¹é… <a> æ ‡ç­¾ä¸­çš„æ ‡é¢˜é“¾æ¥
    pattern = r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>([^<]{8,80})</a>'
    seen = set()
    for match in re.finditer(pattern, html_text):
        link, title = match.group(1), match.group(2)
        title = _clean(title)
        # è¿‡æ»¤éæ–°é—»é“¾æ¥
        if not title or len(title) < 8:
            continue
        if any(skip in title for skip in ('é¦–é¡µ', 'ç™»å½•', 'æ³¨å†Œ', 'å…³äºæˆ‘ä»¬', 'è”ç³»', 'å¹¿å‘Š', 'ä¸‹è½½', 'å®¢æˆ·ç«¯', 'åé¦ˆ')):
            continue
        # è¡¥å…¨ç›¸å¯¹é“¾æ¥
        if link.startswith('//'):
            link = 'https:' + link
        elif link.startswith('/'):
            from urllib.parse import urljoin
            link = urljoin(base_url, link)
        elif not link.startswith('http'):
            continue
        if title in seen:
            continue
        seen.add(title)
        items.append({
            "title": title,
            "link": link,
            "source": source_name,
            "time": "",
            "summary": "",
        })
    return items


# â”€â”€ æ–°æµª API è§£æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_sina_api(text, source_name):
    """è§£ææ–°æµªæ»šåŠ¨æ–°é—» API è¿”å›"""
    items = []
    try:
        # å»æ‰ JSONP callback
        text = re.sub(r'^[^{]*', '', text)
        text = re.sub(r'[^}]*$', '', text)
        data = json.loads(text)
        for item in data.get('result', {}).get('data', []):
            title = item.get('title', '')
            link = item.get('url', '')
            ctime = item.get('ctime', '')
            if title and link:
                items.append({
                    "title": _clean(title),
                    "link": link,
                    "source": source_name,
                    "time": ctime,
                    "summary": _clean(item.get('intro', ''))[:200],
                })
    except (json.JSONDecodeError, KeyError):
        pass
    return items


# â”€â”€ æ ¸å¿ƒé€»è¾‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _fetch_one_source(src):
    """è·å–å¹¶è§£æå•ä¸ªæ–°é—»æºï¼ˆç”¨äºå¹¶å‘ï¼‰"""
    print(f"  ğŸ“¡ æ­£åœ¨è·å– {src['name']}...", file=sys.stderr)
    text = fetch(src["url"])
    if not text:
        return []
    if src["type"] == "rss":
        if 'feed.mix.sina.com.cn' in src["url"]:
            return parse_sina_api(text, src["name"])
        else:
            return parse_rss(text, src["name"])
    else:
        return parse_web_titles(text, src["name"], src["url"])


def fetch_news(category="hot", keyword=None, limit=10):
    """è·å–æŒ‡å®šåˆ†ç±»çš„æ–°é—»ï¼ˆå¹¶å‘è¯·æ±‚æ‰€æœ‰æºï¼‰"""
    from concurrent.futures import ThreadPoolExecutor, as_completed

    sources = SOURCES.get(category, SOURCES["hot"])
    all_items = []

    with ThreadPoolExecutor(max_workers=min(len(sources), 8)) as pool:
        futures = {pool.submit(_fetch_one_source, src): src for src in sources}
        for future in as_completed(futures):
            try:
                all_items.extend(future.result())
            except Exception as e:
                src = futures[future]
                print(f"  âš ï¸  {src['name']} å¼‚å¸¸: {e}", file=sys.stderr)

    # å»é‡ï¼ˆæŒ‰æ ‡é¢˜ï¼‰
    seen = set()
    unique = []
    for item in all_items:
        if item["title"] not in seen:
            seen.add(item["title"])
            unique.append(item)

    # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰ï¼Œæ²¡æœ‰æ—¶é—´çš„æ”¾åœ¨åé¢
    def sort_key(item):
        time_str = item.get("time", "")
        if not time_str:
            return "99-99 99:99"  # æ²¡æœ‰æ—¶é—´çš„æ’åœ¨æœ€å
        return time_str
    
    unique.sort(key=sort_key, reverse=True)

    # å…³é”®è¯è¿‡æ»¤
    if keyword:
        keyword_lower = keyword.lower()
        unique = [i for i in unique if keyword_lower in i["title"].lower() or keyword_lower in i.get("summary", "").lower()]

    # å¤šæ ·æ€§ä¼˜åŒ–ï¼šäº¤é”™å±•ç¤ºä¸åŒæ¥æºï¼Œé¿å…å•ä¸€æ¥æºå æ®å‰æ’
    if len(unique) > limit:
        diversified = []
        source_indices = {}  # è®°å½•æ¯ä¸ªæ¥æºçš„å½“å‰ç´¢å¼•
        
        # æŒ‰æ¥æºåˆ†ç»„
        by_source = {}
        for item in unique:
            source = item.get("source", "Unknown")
            if source not in by_source:
                by_source[source] = []
            by_source[source].append(item)
        
        # è½®æµä»å„ä¸ªæ¥æºå–æ–°é—»ï¼Œå®ç°å¤šæ ·æ€§
        while len(diversified) < limit and by_source:
            for source in list(by_source.keys()):
                if by_source[source]:
                    diversified.append(by_source[source].pop(0))
                    if len(diversified) >= limit:
                        break
                else:
                    del by_source[source]
        
        return diversified
    
    return unique[:limit]


def format_news(items, category="hot", summary_len=100):
    """æ ¼å¼åŒ–æ–°é—»è¾“å‡ºï¼Œsummary_len æ§åˆ¶æ‘˜è¦æ˜¾ç¤ºé•¿åº¦"""
    cat_name = CAT_NAMES.get(category, category)
    lines = [f"ğŸ“° {cat_name}æ–°é—» (å…± {len(items)} æ¡)\n"]
    for i, item in enumerate(items, 1):
        time_str = f" | {item['time']}" if item.get('time') else ""
        lines.append(f"  {i}. {item['title']}")
        lines.append(f"     ğŸ“Œ {item['source']}{time_str}")
        lines.append(f"     ğŸ”— {item['link']}")
        if item.get('summary') and summary_len != 0:
            text = item['summary']
            if summary_len > 0 and len(text) > summary_len:
                text = text[:summary_len] + "..."
            lines.append(f"     ğŸ“ {text}")
        lines.append("")
    return "\n".join(lines)


# â”€â”€ å‘½ä»¤å¤„ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cmd_hot(args):
    """è·å–çƒ­ç‚¹æ–°é—»"""
    items = fetch_news("hot", keyword=args.keyword, limit=args.limit)
    if not items:
        if args.keyword:
            print(f'æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ "{args.keyword}" çš„æ–°é—»')
            print(f"\nğŸ’¡ æç¤ºï¼š")
            print(f"  - RSSæºåªåŒ…å«æœ€æ–°çƒ­ç‚¹æ–°é—»ï¼Œå¯èƒ½ä¸åŒ…å«ç‰¹å®šå…³é”®è¯")
            print(f'  - å»ºè®®ä½¿ç”¨ç™¾åº¦æœç´¢æŸ¥æ‰¾ "{args.keyword}" ç›¸å…³æ–°é—»')
            print(f"  - æˆ–è€…å»æ‰å…³é”®è¯æŸ¥çœ‹æ‰€æœ‰çƒ­ç‚¹æ–°é—»")
        else:
            print("æš‚æ— æ–°é—»æ•°æ®ï¼Œè¯·ç¨åé‡è¯•")
        return
    if args.json:
        print(json.dumps(items, ensure_ascii=False, indent=2))
    else:
        print(format_news(items, "hot", summary_len=args.detail))


def cmd_category(args):
    """æŒ‰åˆ†ç±»è·å–æ–°é—»"""
    cat = args.cat
    if cat not in SOURCES:
        print(f"ä¸æ”¯æŒçš„åˆ†ç±»: {cat}", file=sys.stderr)
        print(f"æ”¯æŒçš„åˆ†ç±»: {', '.join(SOURCES.keys())}", file=sys.stderr)
        sys.exit(1)
    items = fetch_news(cat, keyword=args.keyword, limit=args.limit)
    if not items:
        cat_name = CAT_NAMES.get(cat, cat)
        if args.keyword:
            print(f'æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ "{args.keyword}" çš„{cat_name}æ–°é—»')
            print(f"\nğŸ’¡ æç¤ºï¼š")
            print(f"  - RSSæºåªåŒ…å«æœ€æ–°æ–°é—»ï¼Œå¯èƒ½ä¸åŒ…å«ç‰¹å®šå…³é”®è¯")
            print(f'  - å»ºè®®ä½¿ç”¨ç™¾åº¦æœç´¢æŸ¥æ‰¾ "{args.keyword}" ç›¸å…³æ–°é—»')
            print(f"  - æˆ–è€…å»æ‰å…³é”®è¯æŸ¥çœ‹æ‰€æœ‰{cat_name}æ–°é—»")
        else:
            print(f"æš‚æ—  {cat_name} æ–°é—»æ•°æ®ï¼Œè¯·ç¨åé‡è¯•")
        return
    if args.json:
        print(json.dumps(items, ensure_ascii=False, indent=2))
    else:
        print(format_news(items, cat, summary_len=args.detail))


def cmd_sources(args):
    """åˆ—å‡ºæ‰€æœ‰æ–°é—»æº"""
    print("ğŸ“¡ æ”¯æŒçš„æ–°é—»åˆ†ç±»å’Œæ¥æº:\n")
    for cat, sources in SOURCES.items():
        cat_name = CAT_NAMES.get(cat, cat)
        names = ", ".join(s["name"] for s in sources)
        print(f"  {cat_name} ({cat}): {names}")
    print()


def main():
    parser = argparse.ArgumentParser(
        description='ä¸­æ–‡æ–°é—»èšåˆå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s hot                          è·å–çƒ­ç‚¹æ–°é—»
  %(prog)s hot --keyword AI             æœç´¢ AI ç›¸å…³æ–°é—»
  %(prog)s category --cat tech          è·å–ç§‘æŠ€æ–°é—»
  %(prog)s category --cat finance       è·å–è´¢ç»æ–°é—»
  %(prog)s sources                      æŸ¥çœ‹æ‰€æœ‰æ–°é—»æº
""")
    subparsers = parser.add_subparsers(dest='command', help='å‘½ä»¤')

    hp = subparsers.add_parser('hot', help='è·å–çƒ­ç‚¹æ–°é—»')
    hp.add_argument('--keyword', '-k', help='å…³é”®è¯è¿‡æ»¤')
    hp.add_argument('--limit', '-n', type=int, default=10, help='è¿”å›æ¡æ•°ï¼ˆé»˜è®¤ 10ï¼‰')
    hp.add_argument('--detail', '-d', type=int, default=100, help='æ‘˜è¦é•¿åº¦ï¼ˆé»˜è®¤ 100ï¼Œ0=ä¸æ˜¾ç¤ºï¼Œ-1=å…¨æ–‡ï¼‰')
    hp.add_argument('--json', action='store_true', help='JSON æ ¼å¼è¾“å‡º')

    cp = subparsers.add_parser('category', help='æŒ‰åˆ†ç±»è·å–æ–°é—»')
    cp.add_argument('--cat', '-c', required=True, choices=list(SOURCES.keys()), help='æ–°é—»åˆ†ç±»')
    cp.add_argument('--keyword', '-k', help='å…³é”®è¯è¿‡æ»¤')
    cp.add_argument('--limit', '-n', type=int, default=10, help='è¿”å›æ¡æ•°ï¼ˆé»˜è®¤ 10ï¼‰')
    cp.add_argument('--detail', '-d', type=int, default=100, help='æ‘˜è¦é•¿åº¦ï¼ˆé»˜è®¤ 100ï¼Œ0=ä¸æ˜¾ç¤ºï¼Œ-1=å…¨æ–‡ï¼‰')
    cp.add_argument('--json', action='store_true', help='JSON æ ¼å¼è¾“å‡º')

    subparsers.add_parser('sources', help='æŸ¥çœ‹æ‰€æœ‰æ–°é—»æº')

    args = parser.parse_args()
    if not args.command:
        parser.print_help()
        sys.exit(1)

    {'hot': cmd_hot, 'category': cmd_category, 'sources': cmd_sources}[args.command](args)


if __name__ == '__main__':
    main()
