# LLM Providers æ¨¡å— - æµå¼ä¼˜å…ˆè®¾è®¡

## ğŸš€ æ¦‚è¿°

LLM Providers æ¨¡å—æä¾›äº†**æµå¼ä¼˜å…ˆ**çš„ç»Ÿä¸€æ¥å£æ¥è®¿é—®å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹æä¾›å•†ã€‚ç›¸æ¯”ä¼ ç»Ÿçš„éæµå¼è®¾è®¡ï¼Œæˆ‘ä»¬çš„å®ç°æä¾›äº†æ›´å¥½çš„ç”¨æˆ·ä½“éªŒå’Œæ›´ç°ä»£åŒ–çš„ APIã€‚

### âœ¨ æ ¸å¿ƒä¼˜åŠ¿

1. **çœŸæ­£çš„æµå¼å“åº”** - é€å­—è¾“å‡ºï¼Œå®æ—¶åé¦ˆ
2. **ç°ä»£åŒ–æ•°æ®ç±»** - ä½¿ç”¨ `@dataclass`ï¼Œä»£ç ç®€æ´
3. **10+ æä¾›å•†æ”¯æŒ** - ç»Ÿä¸€æ¥å£è®¿é—®æ‰€æœ‰ä¸»æµ LLM
4. **æ™ºèƒ½æ¨¡å‹è§„èŒƒåŒ–** - è‡ªåŠ¨æ·»åŠ æ­£ç¡®çš„æä¾›å•†å‰ç¼€
5. **å·¥å…·è°ƒç”¨å¢é‡å¤„ç†** - æ­£ç¡®å¤„ç†æµå¼å·¥å…·è°ƒç”¨
6. **å®Œå–„çš„é”™è¯¯å¤„ç†** - é”™è¯¯ä¿¡æ¯åŒ…å«åœ¨æµä¸­
7. **ç±»å‹å®‰å…¨** - å®Œæ•´çš„ç±»å‹æ³¨è§£
8. **æ¸…æ™°çš„ API** - `chat_stream()` æ–¹æ³•åæ˜ç¡®è¡¨è¾¾æ„å›¾

## ğŸ“¦ æ¨¡å—ç»“æ„

```
providers/
â”œâ”€â”€ __init__.py              # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ base.py                  # LLMProvider æŠ½è±¡åŸºç±» + æ•°æ®ç±»
â”œâ”€â”€ litellm_provider.py      # LiteLLM Provider å®ç°
â”œâ”€â”€ transcription.py         # è¯­éŸ³è½¬å½•æœåŠ¡
â””â”€â”€ README.md               # æœ¬æ–‡æ¡£
```

## ğŸ¯ æ ¸å¿ƒç»„ä»¶

### 1. æ•°æ®ç±» (base.py)

#### ToolCall

è¡¨ç¤º LLM è¯·æ±‚è°ƒç”¨çš„å·¥å…·ï¼š

```python
from backend.modules.providers import ToolCall

tool = ToolCall(
    id="call_abc123",
    name="get_weather",
    arguments={"city": "åŒ—äº¬", "unit": "celsius"}
)
```

#### StreamChunk

æµå¼å“åº”å—ï¼Œæ”¯æŒå¤šç§ç±»å‹çš„æ•°æ®ï¼š

```python
from backend.modules.providers import StreamChunk

# å†…å®¹å—
content_chunk = StreamChunk(content="ä½ å¥½")
assert content_chunk.is_content == True

# å·¥å…·è°ƒç”¨å—
tool_chunk = StreamChunk(tool_call=tool)
assert tool_chunk.is_tool_call == True

# å®Œæˆå—
done_chunk = StreamChunk(
    finish_reason="stop",
    usage={"total_tokens": 100}
)
assert done_chunk.is_done == True

# é”™è¯¯å—
error_chunk = StreamChunk(error="API é”™è¯¯")
assert error_chunk.is_error == True
```

**StreamChunk å±æ€§:**
- `content`: æ–‡æœ¬å†…å®¹å¢é‡ï¼ˆé€å­—è¾“å‡ºï¼‰
- `tool_call`: å®Œæ•´çš„å·¥å…·è°ƒç”¨ä¿¡æ¯
- `finish_reason`: å®ŒæˆåŸå›  ("stop", "length", "tool_calls" ç­‰)
- `usage`: Token ä½¿ç”¨ç»Ÿè®¡
- `error`: é”™è¯¯ä¿¡æ¯

**StreamChunk æ–¹æ³•:**
- `is_content`: æ˜¯å¦ä¸ºå†…å®¹å—
- `is_tool_call`: æ˜¯å¦ä¸ºå·¥å…·è°ƒç”¨å—
- `is_done`: æ˜¯å¦ä¸ºå®Œæˆå—
- `is_error`: æ˜¯å¦ä¸ºé”™è¯¯å—

### 2. LLMProvider æŠ½è±¡åŸºç±» (base.py)

æ‰€æœ‰ Provider å¿…é¡»å®ç°çš„æ¥å£ï¼š

```python
from backend.modules.providers import LLMProvider
from typing import AsyncIterator

class MyProvider(LLMProvider):
    async def chat_stream(
        self,
        messages: list[dict],
        tools: list[dict] | None = None,
        model: str | None = None,
        max_tokens: int = 4096,
        temperature: float = 0.7,
        **kwargs
    ) -> AsyncIterator[StreamChunk]:
        # å®ç°æµå¼èŠå¤©
        yield StreamChunk(content="å“åº”å†…å®¹")
        yield StreamChunk(finish_reason="stop")
    
    def get_default_model(self) -> str:
        return "my-model"
```

### 3. LiteLLMProvider (litellm_provider.py)

ä½¿ç”¨ litellm åº“çš„ Provider å®ç°ï¼Œæ”¯æŒ 10+ ä¸ªä¸»æµæä¾›å•†ã€‚

#### æ”¯æŒçš„æä¾›å•†

| æä¾›å•† | æ¨¡å‹ç¤ºä¾‹ | å‰ç¼€ |
|--------|---------|------|
| **OpenRouter** | æ‰€æœ‰æ¨¡å‹ | `openrouter/` |
| **Anthropic** | claude-4.5 | `anthropic/` |
| **OpenAI** | gpt-4, gpt-3.5-turbo | `openai/` |
| **DeepSeek** | deepseek-chat, deepseek-coder | `deepseek/` |
| **Google Gemini** | gemini-pro, gemini-1.5-pro | `gemini/` |
| **Moonshot/Kimi** | moonshot-k2.5, kimi-k2.5 | `moonshot/` |
| **Zhipu GLM** | glm-4, glm-3-turbo | `zai/` |
| **DashScope Qwen** | qwen-turbo, qwen-plus | `dashscope/` |
| **Groq** | llama3-70b, mixtral-8x7b | `groq/` |
| **vLLM** | è‡ªæ‰˜ç®¡æ¨¡å‹ | `hosted_vllm/` |

#### åŸºæœ¬ä½¿ç”¨

```python
from backend.modules.providers import LiteLLMProvider

# åˆå§‹åŒ–
provider = LiteLLMProvider(
    api_key="your_api_key",
    default_model="anthropic/claude-4.5",
    timeout=120.0,
    max_retries=3
)

# æµå¼èŠå¤©
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
    {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹ Python"}
]

async for chunk in provider.chat_stream(messages):
    if chunk.is_content:
        print(chunk.content, end='', flush=True)
    elif chunk.is_tool_call:
        print(f"\nè°ƒç”¨å·¥å…·: {chunk.tool_call.name}")
        print(f"å‚æ•°: {chunk.tool_call.arguments}")
    elif chunk.is_done:
        print(f"\nå®Œæˆ: {chunk.finish_reason}")
        if chunk.usage:
            print(f"Token ä½¿ç”¨: {chunk.usage}")
    elif chunk.is_error:
        print(f"\né”™è¯¯: {chunk.error}")
```

#### å¸¦å·¥å…·è°ƒç”¨çš„ç¤ºä¾‹

```python
# å®šä¹‰å·¥å…·
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "åŸå¸‚åç§°"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "æ¸©åº¦å•ä½"
                    }
                },
                "required": ["city"]
            }
        }
    }
]

messages = [
    {"role": "user", "content": "åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}
]

async for chunk in provider.chat_stream(messages, tools=tools):
    if chunk.is_tool_call:
        tool = chunk.tool_call
        print(f"å·¥å…·: {tool.name}")
        print(f"å‚æ•°: {tool.arguments}")
        
        # æ‰§è¡Œå·¥å…·
        if tool.name == "get_weather":
            result = get_weather(**tool.arguments)
            # å°†ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²...
```

#### å¤šæä¾›å•†é…ç½®

```python
# OpenRouter
provider_or = LiteLLMProvider(
    api_key="sk-or-v1-...",
    default_model="anthropic/claude-4.5"
)

# Anthropic ç›´è¿
provider_anthropic = LiteLLMProvider(
    api_key="sk-ant-...",
    default_model="claude-4.5"
)

# OpenAI
provider_openai = LiteLLMProvider(
    api_key="sk-...",
    default_model="gpt-4"
)

# DeepSeek
provider_deepseek = LiteLLMProvider(
    api_key="sk-...",
    default_model="deepseek-chat"
)

# Moonshot/Kimi
provider_moonshot = LiteLLMProvider(
    api_key="sk-...",
    default_model="moonshot-k2.5"
)

# vLLM è‡ªæ‰˜ç®¡
provider_vllm = LiteLLMProvider(
    api_key="your_key",
    api_base="http://localhost:8000",
    default_model="my-model"
)
```

#### æ™ºèƒ½æ¨¡å‹åç§°è§„èŒƒåŒ–

Provider ä¼šè‡ªåŠ¨ä¸ºæ¨¡å‹åç§°æ·»åŠ æ­£ç¡®çš„å‰ç¼€ï¼š

```python
provider = LiteLLMProvider(api_key="sk-or-test")

# è‡ªåŠ¨æ·»åŠ  openrouter/ å‰ç¼€
provider._normalize_model_name("gpt-4")
# è¿”å›: "openrouter/gpt-4"

provider2 = LiteLLMProvider(api_key="test")

# è‡ªåŠ¨æ·»åŠ  zai/ å‰ç¼€
provider2._normalize_model_name("glm-4")
# è¿”å›: "zai/glm-4"

# è‡ªåŠ¨æ·»åŠ  dashscope/ å‰ç¼€
provider2._normalize_model_name("qwen-turbo")
# è¿”å›: "dashscope/qwen-turbo"
```

### 4. TranscriptionService (transcription.py)

è¯­éŸ³è½¬å½•æœåŠ¡ï¼Œæ”¯æŒ Groq å’Œ OpenAI çš„ Whisper APIã€‚

#### åŸºæœ¬ä½¿ç”¨

```python
from backend.modules.providers import TranscriptionService

# ä½¿ç”¨ Groq Whisper
service = TranscriptionService(
    provider="groq",
    api_key="your_groq_api_key",
    model="whisper-large-v3"
)

# è¯»å–éŸ³é¢‘æ–‡ä»¶
with open("recording.mp3", "rb") as f:
    audio_bytes = f.read()

# è½¬å½•
result = await service.transcribe(
    audio_file=audio_bytes,
    language="zh",  # å¯é€‰ï¼šæŒ‡å®šè¯­è¨€
    prompt="è¿™æ˜¯ä¸€æ®µå…³äº..."  # å¯é€‰ï¼šæç¤ºæ–‡æœ¬
)

print(result["text"])      # è½¬å½•æ–‡æœ¬
print(result["language"])  # æ£€æµ‹åˆ°çš„è¯­è¨€
print(result["duration"])  # éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
```

#### æ”¯æŒçš„éŸ³é¢‘æ ¼å¼

- MP3 (.mp3)
- MP4 (.mp4)
- MPEG (.mpeg, .mpga)
- M4A (.m4a)
- WAV (.wav)
- WebM (.webm)

#### éŸ³é¢‘æ ¼å¼éªŒè¯

```python
from pathlib import Path

service = TranscriptionService(provider="groq", api_key="key")

# éªŒè¯æ ¼å¼
assert service.validate_audio_format(Path("audio.mp3")) == True
assert service.validate_audio_format(Path("audio.txt")) == False
```

## ğŸ”¥ å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºæœ¬æµå¼èŠå¤©

```python
from backend.modules.providers import LiteLLMProvider

async def basic_chat():
    provider = LiteLLMProvider(
        api_key="your_key",
        default_model="gpt-3.5-turbo"
    )
    
    messages = [
        {"role": "user", "content": "å†™ä¸€é¦–å…³äºç¼–ç¨‹çš„è¯—"}
    ]
    
    print("AI: ", end='', flush=True)
    
    async for chunk in provider.chat_stream(messages):
        if chunk.is_content:
            print(chunk.content, end='', flush=True)
        elif chunk.is_done:
            print(f"\n\n[å®Œæˆ: {chunk.finish_reason}]")
            if chunk.usage:
                tokens = chunk.usage.get('total_tokens', 0)
                print(f"[ä½¿ç”¨ {tokens} tokens]")
        elif chunk.is_error:
            print(f"\n[é”™è¯¯: {chunk.error}]")
            break

# è¿è¡Œ
import asyncio
asyncio.run(basic_chat())
```

### ç¤ºä¾‹ 2: å¸¦å·¥å…·è°ƒç”¨çš„æ™ºèƒ½åŠ©æ‰‹

```python
from backend.modules.providers import LiteLLMProvider
import json

async def smart_assistant():
    provider = LiteLLMProvider(api_key="your_key")
    
    # å®šä¹‰å·¥å…·
    tools = [
        {
            "type": "function",
            "function": {
                "name": "search_web",
                "description": "æœç´¢ç½‘ç»œè·å–ä¿¡æ¯",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "æœç´¢æŸ¥è¯¢"}
                    },
                    "required": ["query"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "calculate",
                "description": "æ‰§è¡Œæ•°å­¦è®¡ç®—",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "expression": {"type": "string", "description": "æ•°å­¦è¡¨è¾¾å¼"}
                    },
                    "required": ["expression"]
                }
            }
        }
    ]
    
    messages = [
        {"role": "user", "content": "æœç´¢ä¸€ä¸‹ Python 3.12 çš„æ–°ç‰¹æ€§ï¼Œç„¶åè®¡ç®— 123 * 456"}
    ]
    
    async for chunk in provider.chat_stream(messages, tools=tools):
        if chunk.is_content:
            print(chunk.content, end='', flush=True)
        
        elif chunk.is_tool_call:
            tool = chunk.tool_call
            print(f"\n\n[è°ƒç”¨å·¥å…·: {tool.name}]")
            print(f"[å‚æ•°: {json.dumps(tool.arguments, ensure_ascii=False)}]")
            
            # æ‰§è¡Œå·¥å…·
            if tool.name == "search_web":
                result = f"æœç´¢ç»“æœ: Python 3.12 å¼•å…¥äº†..."
            elif tool.name == "calculate":
                result = str(eval(tool.arguments["expression"]))
            
            print(f"[ç»“æœ: {result}]")
            
            # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
            messages.append({
                "role": "assistant",
                "content": None,
                "tool_calls": [{
                    "id": tool.id,
                    "type": "function",
                    "function": {
                        "name": tool.name,
                        "arguments": json.dumps(tool.arguments)
                    }
                }]
            })
            messages.append({
                "role": "tool",
                "tool_call_id": tool.id,
                "content": result
            })
        
        elif chunk.is_done:
            print(f"\n\n[å®Œæˆ: {chunk.finish_reason}]")

asyncio.run(smart_assistant())
```

### ç¤ºä¾‹ 3: å¤šè½®å¯¹è¯

```python
from backend.modules.providers import LiteLLMProvider

async def multi_turn_chat():
    provider = LiteLLMProvider(api_key="your_key")
    
    messages = []
    
    while True:
        # ç”¨æˆ·è¾“å…¥
        user_input = input("\nä½ : ")
        if user_input.lower() in ['é€€å‡º', 'quit', 'exit']:
            break
        
        messages.append({"role": "user", "content": user_input})
        
        # AI å“åº”
        print("AI: ", end='', flush=True)
        assistant_message = ""
        
        async for chunk in provider.chat_stream(messages):
            if chunk.is_content:
                print(chunk.content, end='', flush=True)
                assistant_message += chunk.content
            elif chunk.is_done:
                print()
                break
            elif chunk.is_error:
                print(f"\né”™è¯¯: {chunk.error}")
                break
        
        # æ·»åŠ åˆ°å†å²
        if assistant_message:
            messages.append({"role": "assistant", "content": assistant_message})

asyncio.run(multi_turn_chat())
```

### ç¤ºä¾‹ 4: è¯­éŸ³è½¬å½•

```python
from backend.modules.providers import TranscriptionService

async def transcribe_audio():
    service = TranscriptionService(
        provider="groq",
        api_key="your_groq_key",
        model="whisper-large-v3"
    )
    
    # è¯»å–éŸ³é¢‘
    with open("meeting_recording.mp3", "rb") as f:
        audio_bytes = f.read()
    
    print("æ­£åœ¨è½¬å½•...")
    
    # è½¬å½•
    result = await service.transcribe(
        audio_file=audio_bytes,
        language="zh",
        prompt="è¿™æ˜¯ä¸€æ®µä¼šè®®å½•éŸ³"
    )
    
    print(f"\nè½¬å½•ç»“æœ:\n{result['text']}")
    print(f"\nè¯­è¨€: {result['language']}")
    print(f"æ—¶é•¿: {result['duration']} ç§’")

asyncio.run(transcribe_audio())
```

## âš™ï¸ é…ç½®é€‰é¡¹

### LiteLLMProvider é…ç½®

```python
provider = LiteLLMProvider(
    api_key="your_key",              # API å¯†é’¥
    api_base="https://...",          # è‡ªå®šä¹‰ API ç«¯ç‚¹ï¼ˆå¯é€‰ï¼‰
    default_model="gpt-4",           # é»˜è®¤æ¨¡å‹
    timeout=120.0,                   # è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
    max_retries=3,                   # æœ€å¤§é‡è¯•æ¬¡æ•°
)
```

### TranscriptionService é…ç½®

```python
service = TranscriptionService(
    provider="groq",                 # æä¾›å•†: groq, openai
    api_key="your_key",              # API å¯†é’¥
    model="whisper-large-v3",        # æ¨¡å‹åç§°
)
```

## ğŸ”§ é”™è¯¯å¤„ç†

æ‰€æœ‰é”™è¯¯éƒ½é€šè¿‡ `StreamChunk` çš„ `error` å­—æ®µè¿”å›ï¼š

```python
async for chunk in provider.chat_stream(messages):
    if chunk.is_error:
        print(f"é”™è¯¯: {chunk.error}")
        # å¤„ç†é”™è¯¯
        break
    # æ­£å¸¸å¤„ç†...
```

## ğŸ“Š ä¾èµ–é¡¹

```txt
litellm>=1.0.0
groq>=0.4.0
openai>=1.0.0
pydub>=0.25.0  # å¯é€‰ï¼Œç”¨äºéŸ³é¢‘æ—¶é•¿æ£€æµ‹
```

## ğŸ§ª æµ‹è¯•

è¿è¡Œæµ‹è¯•ï¼š

```bash
python3 -m pytest tests/backend/test_providers.py -v
```

## ğŸ’¡ æœ€ä½³å®è·µ

1. **API å¯†é’¥å®‰å…¨**: ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ï¼Œä¸è¦ç¡¬ç¼–ç 
2. **é€Ÿç‡é™åˆ¶**: å®ç°é€‚å½“çš„é‡è¯•é€»è¾‘å’Œé€€é¿ç­–ç•¥
3. **æˆæœ¬æ§åˆ¶**: ç›‘æ§ token ä½¿ç”¨é‡ï¼Œè®¾ç½®åˆç†çš„ `max_tokens`
4. **é”™è¯¯å¤„ç†**: å§‹ç»ˆæ£€æŸ¥ `chunk.is_error`ï¼Œå®ç°ä¼˜é›…é™çº§
5. **æµå¼ä½“éªŒ**: ä½¿ç”¨ `flush=True` ç¡®ä¿å®æ—¶è¾“å‡º
6. **å·¥å…·è°ƒç”¨**: æ­£ç¡®å¤„ç†å·¥å…·è°ƒç”¨ç»“æœï¼Œæ·»åŠ åˆ°æ¶ˆæ¯å†å²

## ğŸš€ æ‰©å±•

è¦æ·»åŠ æ–°çš„ Providerï¼š

```python
from backend.modules.providers import LLMProvider, StreamChunk
from typing import AsyncIterator

class MyCustomProvider(LLMProvider):
    async def chat_stream(
        self,
        messages: list[dict],
        tools: list[dict] | None = None,
        model: str | None = None,
        max_tokens: int = 4096,
        temperature: float = 0.7,
        **kwargs
    ) -> AsyncIterator[StreamChunk]:
        # å®ç°ä½ çš„æµå¼é€»è¾‘
        yield StreamChunk(content="å“åº”å†…å®¹")
        yield StreamChunk(finish_reason="stop")
    
    def get_default_model(self) -> str:
        return "my-default-model"
```

## ğŸ“„ è®¸å¯è¯

MIT License
